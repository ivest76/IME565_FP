{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from matplotlib import pyplot as plt # Matplotlib\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Package to implement ML Algorithms\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor     # Decision Tree\n",
    "from sklearn.ensemble import RandomForestRegressor # Random Forest\n",
    "\n",
    "# Package for data partitioning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Package for generating confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Package for generating classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Import packages to implement Stratified K-fold CV\n",
    "from sklearn.model_selection import KFold # For creating folds\n",
    "\n",
    "# Import Package to implement GridSearch CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importing package for Randomized Search CV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Package to record time\n",
    "import time\n",
    "\n",
    "# Package for Data pretty printer\n",
    "from pprint import pprint\n",
    "\n",
    "# Module to save and load Python objects to and from files\n",
    "import pickle \n",
    "\n",
    "# Ignore Deprecation Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display inline plots as vector-based (svg)\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>urban_perc_state</th>\n",
       "      <th>County</th>\n",
       "      <th>percentage20_Donald_Trump</th>\n",
       "      <th>percentage20_Joe_Biden</th>\n",
       "      <th>avg_sal_2022</th>\n",
       "      <th>white_perc</th>\n",
       "      <th>baa_perc</th>\n",
       "      <th>asian_perc</th>\n",
       "      <th>native_perc</th>\n",
       "      <th>pi_perc</th>\n",
       "      <th>his_perc</th>\n",
       "      <th>AQI</th>\n",
       "      <th>CO_perc</th>\n",
       "      <th>NO2_perc</th>\n",
       "      <th>O3_perc</th>\n",
       "      <th>PM2.5_perc</th>\n",
       "      <th>PM10_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>0.577</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.223</td>\n",
       "      <td>56747.0</td>\n",
       "      <td>0.893656</td>\n",
       "      <td>0.094556</td>\n",
       "      <td>0.017246</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.050362</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>0.577</td>\n",
       "      <td>Clay</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.183</td>\n",
       "      <td>39876.0</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.153050</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>0.577</td>\n",
       "      <td>DeKalb</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.146</td>\n",
       "      <td>40558.0</td>\n",
       "      <td>0.946637</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.159921</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925620</td>\n",
       "      <td>0.074380</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>0.577</td>\n",
       "      <td>Elmore</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.252</td>\n",
       "      <td>49071.0</td>\n",
       "      <td>0.764367</td>\n",
       "      <td>0.230173</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.031799</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>0.577</td>\n",
       "      <td>Etowah</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.242</td>\n",
       "      <td>42951.0</td>\n",
       "      <td>0.825877</td>\n",
       "      <td>0.163812</td>\n",
       "      <td>0.011262</td>\n",
       "      <td>0.015201</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.047736</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.424581</td>\n",
       "      <td>0.575419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     State  urban_perc_state   County  percentage20_Donald_Trump  \\\n",
       "0  Alabama             0.577  Baldwin                      0.762   \n",
       "1  Alabama             0.577     Clay                      0.808   \n",
       "2  Alabama             0.577   DeKalb                      0.844   \n",
       "3  Alabama             0.577   Elmore                      0.736   \n",
       "4  Alabama             0.577   Etowah                      0.745   \n",
       "\n",
       "   percentage20_Joe_Biden  avg_sal_2022  white_perc  baa_perc  asian_perc  \\\n",
       "0                   0.223       56747.0    0.893656  0.094556    0.017246   \n",
       "1                   0.183       39876.0    0.851035  0.153050    0.004719   \n",
       "2                   0.146       40558.0    0.946637  0.026404    0.007236   \n",
       "3                   0.252       49071.0    0.764367  0.230173    0.011433   \n",
       "4                   0.242       42951.0    0.825877  0.163812    0.011262   \n",
       "\n",
       "   native_perc   pi_perc  his_perc  AQI  CO_perc  NO2_perc   O3_perc  \\\n",
       "0     0.017071  0.002414  0.050362   40      0.0       0.0  0.808511   \n",
       "1     0.012326  0.001056  0.031835   27      0.0       0.0  0.000000   \n",
       "2     0.037126  0.006820  0.159921   37      0.0       0.0  0.925620   \n",
       "3     0.011121  0.001999  0.031799   37      0.0       0.0  1.000000   \n",
       "4     0.015201  0.004152  0.047736   42      0.0       0.0  0.424581   \n",
       "\n",
       "   PM2.5_perc  PM10_perc  \n",
       "0    0.191489        0.0  \n",
       "1    1.000000        0.0  \n",
       "2    0.074380        0.0  \n",
       "3    0.000000        0.0  \n",
       "4    0.575419        0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqi = pd.read_csv(\"mlm_aqi_data.csv\")\n",
    "aqi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aqi.drop(columns= [\"AQI\"])\n",
    "y = aqi['AQI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urban_perc_state</th>\n",
       "      <th>percentage20_Donald_Trump</th>\n",
       "      <th>percentage20_Joe_Biden</th>\n",
       "      <th>avg_sal_2022</th>\n",
       "      <th>white_perc</th>\n",
       "      <th>baa_perc</th>\n",
       "      <th>asian_perc</th>\n",
       "      <th>native_perc</th>\n",
       "      <th>pi_perc</th>\n",
       "      <th>his_perc</th>\n",
       "      <th>...</th>\n",
       "      <th>State_South Dakota</th>\n",
       "      <th>State_Tennessee</th>\n",
       "      <th>State_Texas</th>\n",
       "      <th>State_Utah</th>\n",
       "      <th>State_Vermont</th>\n",
       "      <th>State_Virginia</th>\n",
       "      <th>State_Washington</th>\n",
       "      <th>State_West Virginia</th>\n",
       "      <th>State_Wisconsin</th>\n",
       "      <th>State_Wyoming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.223</td>\n",
       "      <td>56747.0</td>\n",
       "      <td>0.893656</td>\n",
       "      <td>0.094556</td>\n",
       "      <td>0.017246</td>\n",
       "      <td>0.017071</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.050362</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.183</td>\n",
       "      <td>39876.0</td>\n",
       "      <td>0.851035</td>\n",
       "      <td>0.153050</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.146</td>\n",
       "      <td>40558.0</td>\n",
       "      <td>0.946637</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>0.007236</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.159921</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.252</td>\n",
       "      <td>49071.0</td>\n",
       "      <td>0.764367</td>\n",
       "      <td>0.230173</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.031799</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.242</td>\n",
       "      <td>42951.0</td>\n",
       "      <td>0.825877</td>\n",
       "      <td>0.163812</td>\n",
       "      <td>0.011262</td>\n",
       "      <td>0.015201</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>0.047736</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 823 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   urban_perc_state  percentage20_Donald_Trump  percentage20_Joe_Biden  \\\n",
       "0             0.577                      0.762                   0.223   \n",
       "1             0.577                      0.808                   0.183   \n",
       "2             0.577                      0.844                   0.146   \n",
       "3             0.577                      0.736                   0.252   \n",
       "4             0.577                      0.745                   0.242   \n",
       "\n",
       "   avg_sal_2022  white_perc  baa_perc  asian_perc  native_perc   pi_perc  \\\n",
       "0       56747.0    0.893656  0.094556    0.017246     0.017071  0.002414   \n",
       "1       39876.0    0.851035  0.153050    0.004719     0.012326  0.001056   \n",
       "2       40558.0    0.946637  0.026404    0.007236     0.037126  0.006820   \n",
       "3       49071.0    0.764367  0.230173    0.011433     0.011121  0.001999   \n",
       "4       42951.0    0.825877  0.163812    0.011262     0.015201  0.004152   \n",
       "\n",
       "   his_perc  ...  State_South Dakota  State_Tennessee  State_Texas  \\\n",
       "0  0.050362  ...               False            False        False   \n",
       "1  0.031835  ...               False            False        False   \n",
       "2  0.159921  ...               False            False        False   \n",
       "3  0.031799  ...               False            False        False   \n",
       "4  0.047736  ...               False            False        False   \n",
       "\n",
       "   State_Utah  State_Vermont  State_Virginia  State_Washington  \\\n",
       "0       False          False           False             False   \n",
       "1       False          False           False             False   \n",
       "2       False          False           False             False   \n",
       "3       False          False           False             False   \n",
       "4       False          False           False             False   \n",
       "\n",
       "   State_West Virginia  State_Wisconsin  State_Wyoming  \n",
       "0                False            False          False  \n",
       "1                False            False          False  \n",
       "2                False            False          False  \n",
       "3                False            False          False  \n",
       "4                False            False          False  \n",
       "\n",
       "[5 rows x 823 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding for categorical variables\n",
    "cat_var = [\"County\", \"State\"]\n",
    "X_encoded = pd.get_dummies(X, columns = cat_var)\n",
    "\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partitioning into train and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(X_encoded, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model\n",
    "#regressor = RandomForestRegressor(random_state = 42)\n",
    "regressor = sklearn.ensemble.HistGradientBoostingRegressor(random_state=42)\n",
    "folds = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 15, 25, 40, 60],\n",
    "    'min_samples_leaf': [5, 15, 25, 40, 60, 100]\n",
    "    #'min_samples_split': [5, 15, 25, 40, 60, 100]\n",
    "    #'n_estimators': [40, 60, 100, 200, 300]\n",
    "    }\n",
    "\n",
    "#randomized for before hyperparameter tuning\n",
    "random_cv = RandomizedSearchCV(estimator = regressor,\n",
    "                              param_distributions = param_grid,\n",
    "                              n_iter = 100,\n",
    "                              scoring = 'r2',\n",
    "                              cv = folds,\n",
    "                              verbose = 2,\n",
    "                              random_state = 42,\n",
    "                              n_jobs = -1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=15; total time=   8.0s\n",
      "[CV] END ....................max_depth=5, min_samples_leaf=5; total time=   9.5s\n",
      "[CV] END ....................max_depth=5, min_samples_leaf=5; total time=   9.7s\n",
      "[CV] END ....................max_depth=5, min_samples_leaf=5; total time=   9.6s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=15; total time=   6.3s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=25; total time=   5.4s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=25; total time=   5.8s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=15; total time=   6.7s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=40; total time=   4.1s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=25; total time=   4.6s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=40; total time=   4.1s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=40; total time=   3.8s\n",
      "[CV] END ..................max_depth=5, min_samples_leaf=100; total time=   1.9s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=60; total time=   2.9s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=60; total time=   2.8s\n",
      "[CV] END ...................max_depth=5, min_samples_leaf=60; total time=   2.9s\n",
      "[CV] END ..................max_depth=5, min_samples_leaf=100; total time=   1.9s\n",
      "[CV] END ..................max_depth=5, min_samples_leaf=100; total time=   2.0s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=15; total time=  18.2s\n",
      "[CV] END ...................max_depth=15, min_samples_leaf=5; total time=  22.5s\n",
      "[CV] END ...................max_depth=15, min_samples_leaf=5; total time=  22.5s\n",
      "[CV] END ...................max_depth=15, min_samples_leaf=5; total time=  22.9s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=25; total time=   9.2s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=25; total time=   9.2s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=15; total time=  14.2s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=15; total time=  14.5s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=40; total time=  10.3s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=40; total time=  10.6s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=25; total time=  14.7s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=40; total time=  12.6s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=60; total time=   7.3s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=60; total time=   6.5s\n",
      "[CV] END ..................max_depth=15, min_samples_leaf=60; total time=   5.6s\n",
      "[CV] END .................max_depth=15, min_samples_leaf=100; total time=   3.1s\n",
      "[CV] END .................max_depth=15, min_samples_leaf=100; total time=   3.3s\n",
      "[CV] END .................max_depth=15, min_samples_leaf=100; total time=   3.6s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=15; total time=  15.6s\n",
      "[CV] END ...................max_depth=25, min_samples_leaf=5; total time=  21.9s\n",
      "[CV] END ...................max_depth=25, min_samples_leaf=5; total time=  22.3s\n",
      "[CV] END ...................max_depth=25, min_samples_leaf=5; total time=  21.5s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=25; total time=   8.2s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=25; total time=   7.8s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=15; total time=  14.7s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=15; total time=  13.3s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=40; total time=   5.1s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=40; total time=   4.9s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=25; total time=   7.8s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=40; total time=   5.0s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=60; total time=   3.5s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=60; total time=   3.3s\n",
      "[CV] END ..................max_depth=25, min_samples_leaf=60; total time=   3.4s\n",
      "[CV] END .................max_depth=25, min_samples_leaf=100; total time=   1.9s\n",
      "[CV] END .................max_depth=25, min_samples_leaf=100; total time=   2.0s\n",
      "[CV] END .................max_depth=25, min_samples_leaf=100; total time=   2.0s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=15; total time=  24.9s\n",
      "[CV] END ...................max_depth=40, min_samples_leaf=5; total time=  28.8s\n",
      "[CV] END ...................max_depth=40, min_samples_leaf=5; total time=  29.2s\n",
      "[CV] END ...................max_depth=40, min_samples_leaf=5; total time=  29.4s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=25; total time=   9.1s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=25; total time=   9.1s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=15; total time=  14.6s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=15; total time=  16.0s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=40; total time=   7.0s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=40; total time=   6.7s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=25; total time=   9.8s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=60; total time=   3.8s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=40; total time=   5.4s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=60; total time=   3.6s\n",
      "[CV] END .................max_depth=40, min_samples_leaf=100; total time=   2.2s\n",
      "[CV] END ..................max_depth=40, min_samples_leaf=60; total time=   3.7s\n",
      "[CV] END .................max_depth=40, min_samples_leaf=100; total time=   2.1s\n",
      "[CV] END .................max_depth=40, min_samples_leaf=100; total time=   2.3s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=15; total time=  16.6s\n",
      "[CV] END ...................max_depth=60, min_samples_leaf=5; total time=  20.1s\n",
      "[CV] END ...................max_depth=60, min_samples_leaf=5; total time=  20.9s\n",
      "[CV] END ...................max_depth=60, min_samples_leaf=5; total time=  21.2s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=25; total time=  13.2s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=25; total time=  13.9s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=15; total time=  20.6s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=15; total time=  20.7s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=40; total time=   6.8s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=40; total time=   5.4s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=25; total time=   9.6s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=60; total time=   3.8s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=40; total time=   5.4s\n",
      "[CV] END .................max_depth=60, min_samples_leaf=100; total time=   2.1s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=60; total time=   3.5s\n",
      "[CV] END ..................max_depth=60, min_samples_leaf=60; total time=   3.4s\n",
      "[CV] END .................max_depth=60, min_samples_leaf=100; total time=   1.9s\n",
      "[CV] END .................max_depth=60, min_samples_leaf=100; total time=   1.2s\n",
      "Training time: 227.887216091156s\n",
      "Initial score:  0.6909289709388959\n",
      "Initial parameters:  {'min_samples_leaf': 5, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "random_cv.fit(train_X, train_y)\n",
    "stop = time.time()            \n",
    "print(f\"Training time: {stop - start}s\")\n",
    "print('Initial score: ', random_cv.best_score_)\n",
    "print('Initial parameters: ', random_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#generating final model w GridSearchCV()\n",
    "param_grid = {\n",
    "    'max_depth': [],\n",
    "    'min_samples_leaf': [],\n",
    "    'min_samples_split': [],\n",
    "    'n_estimators': []\n",
    "    }\n",
    "\n",
    "model_cv = GridSearchCV(estimator = regressor, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "start = time.time()\n",
    "\n",
    "#fitting random forest model\n",
    "model_cv.fit(train_X, train_y)  \n",
    "stop = time.time()            \n",
    "print(f\"Training time: {stop - start}s\")\n",
    "print('Improved score: ', model_cv.best_score_)\n",
    "print('Improved parameters: ', model_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Storing importance values from the trained model\n",
    "importance = model_cv.best_estimator_.feature_importances_\n",
    "\n",
    "# Storing feature importance as a dataframe\n",
    "feature_imp = pd.DataFrame(list(zip(train_X.columns, importance)),\n",
    "               columns = ['Feature', 'Importance'])\n",
    "\n",
    "feature_imp = feature_imp.sort_values('Importance', ascending = False).reset_index(drop = True)\n",
    "feature_imp = feature_imp[feature_imp['Importance'] > 0.001]\n",
    "# Bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_imp['Feature'], feature_imp['Importance'], color = ['mediumslateblue', 'deeppink'])\n",
    "\n",
    "plt.xlabel(\"Importance\", fontsize = 12)\n",
    "plt.ylabel(\"Input Feature\", fontsize = 12)\n",
    "plt.title('Which features are the most important for air quality prediction?', fontsize = 12) \n",
    "plt.yticks(fontsize = 8) # fontsize of yticks\n",
    "plt.xticks(fontsize = 8) # fontsize of xticks\n",
    "plt.savefig(\"rf_feature_imp.svg\", bbox_inches=\"tight\")\n",
    "plt.tight_layout();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Creating the file where we want to write the model\n",
    "rf_pickle = open('rf_aqi.pickle', 'wb') \n",
    "\n",
    "# Write RF model to the file\n",
    "pickle.dump(model_cv, rf_pickle) \n",
    "\n",
    "# Close the file\n",
    "rf_pickle.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest model\n",
    "regressor = DecisionTreeRegressor(random_state = 42)\n",
    "folds = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "'''\n",
    "param_grid = {\n",
    "    'max_depth': [5, 15, 25, 40, 60],\n",
    "    'min_samples_leaf': [5, 15, 25, 40, 60, 100],\n",
    "    'min_samples_split': [5, 15, 25, 40, 60, 100]\n",
    "    }\n",
    "\n",
    "#randomized for before hyperparameter tuning\n",
    "random_cv = RandomizedSearchCV(estimator = regressor,\n",
    "                              param_distributions = param_grid,\n",
    "                              n_iter = 100,\n",
    "                              scoring = 'r2',\n",
    "                              cv = folds,\n",
    "                              verbose = 2,\n",
    "                              random_state = 42,\n",
    "                              n_jobs = -1)\n",
    "\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_cv.fit(train_X, train_y)  \n",
    "stop = time.time()            \n",
    "print(f\"Training time: {stop - start}s\")\n",
    "print('Initial score: ', model_cv.best_score_)\n",
    "print('Initial parameters: ', model_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#generating final model w GridSearchCV()\n",
    "param_grid = {\n",
    "    'max_depth': [],\n",
    "    'min_samples_leaf': [],\n",
    "    'min_samples_split': [],\n",
    "    'n_estimators': []\n",
    "\n",
    "model_cv = GridSearchCV(estimator = regressor, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "start = time.time()\n",
    "\n",
    "#fitting random forest model\n",
    "model_cv.fit(train_X, train_y)  \n",
    "stop = time.time()            \n",
    "print(f\"Training time: {stop - start}s\")\n",
    "print('Improved score: ', model_cv.best_score_)\n",
    "print('Improved parameters: ', model_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Storing importance values from the trained model\n",
    "importance = model_cv.best_estimator_.feature_importances_\n",
    "\n",
    "# Storing feature importance as a dataframe\n",
    "feature_imp = pd.DataFrame(list(zip(train_X.columns, importance)),\n",
    "               columns = ['Feature', 'Importance'])\n",
    "\n",
    "feature_imp = feature_imp.sort_values('Importance', ascending = False).reset_index(drop = True)\n",
    "feature_imp = feature_imp[feature_imp['Importance'] > 0.001]\n",
    "# Bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_imp['Feature'], feature_imp['Importance'], color = ['mediumslateblue', 'deeppink'])\n",
    "\n",
    "plt.xlabel(\"Importance\", fontsize = 12)\n",
    "plt.ylabel(\"Input Feature\", fontsize = 12)\n",
    "plt.title('Which features are the most important for air quality prediction?', fontsize = 12) \n",
    "plt.yticks(fontsize = 8) # fontsize of yticks\n",
    "plt.xticks(fontsize = 8) # fontsize of xticks\n",
    "plt.savefig(\"rf_feature_imp.svg\", bbox_inches=\"tight\")\n",
    "plt.tight_layout();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Creating the file where we want to write the model\n",
    "dt_pickle = open('dt_aqi.pickle', 'wb') \n",
    "\n",
    "# Write RF model to the file\n",
    "pickle.dump(model_cv, dt_pickle) \n",
    "\n",
    "# Close the file\n",
    "dt_pickle.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = AdaBoostRegressor(random_state=42)\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_estimators = [int(x) for x in np.linspace(start = 5, stop = 500, num = 10)]\n",
    "\n",
    "learning_rate = [x for x in np.arange(0.1, 2.1, 0.1)]\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate\n",
    "}\n",
    "\n",
    "model_cv = RandomizedSearchCV(estimator = regressor,\n",
    "                              param_distributions = param_grid,\n",
    "                              n_iter = 100,\n",
    "                              scoring = 'f1_macro',\n",
    "                              cv = folds,\n",
    "                              verbose = 2,\n",
    "                              random_state = 42,\n",
    "                              n_jobs = -1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_cv.fit(train_X, train_y)  \n",
    "stop = time.time()            \n",
    "print(f\"Training time: {stop - start}s\")\n",
    "print('Initial score: ', model_cv.best_score_)\n",
    "print('Initial parameters: ', model_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#best model:\n",
    "#'learning_rate': , 'n_estimators': \n",
    "param_grid = {'n_estimators': [],\n",
    "               'learning_rate': []\n",
    "}\n",
    "\n",
    "# Call GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = regressor, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "# Fit the model\n",
    "start = time.time()            # Start Time\n",
    "model_cv.fit(train_X, train_y)  \n",
    "stop = time.time()             # End Time\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "print('Improved score: ', model_cv.best_score_)\n",
    "print('Improved parameters: ', model_cv.best_params_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Storing importance values from the trained model\n",
    "importance = model_cv.best_estimator_.feature_importances_\n",
    "\n",
    "# Storing feature importance as a dataframe\n",
    "feature_imp = pd.DataFrame(list(zip(train_X.columns, importance)),\n",
    "               columns = ['Feature', 'Importance'])\n",
    "\n",
    "feature_imp = feature_imp.sort_values('Importance', ascending = False).reset_index(drop = True)\n",
    "feature_imp = feature_imp[feature_imp['Importance'] > 0.001]\n",
    "# Bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_imp['Feature'], feature_imp['Importance'], color = ['mediumslateblue', 'deeppink'])\n",
    "\n",
    "plt.xlabel(\"Importance\", fontsize = 12)\n",
    "plt.ylabel(\"Input Feature\", fontsize = 12)\n",
    "plt.title('Which features are the most important for traffic volume prediction?', fontsize = 12) \n",
    "plt.yticks(fontsize = 8) # fontsize of yticks\n",
    "plt.xticks(fontsize = 8) # fontsize of xticks\n",
    "plt.savefig(\"ad_feature_imp.svg\", bbox_inches=\"tight\")\n",
    "plt.tight_layout();\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#saving adaboost model\n",
    "ad_pickle = open('ad_traffic.pickle', 'wb') \n",
    "\n",
    "# Write RF model to the file\n",
    "pickle.dump(model_cv, ad_pickle) \n",
    "\n",
    "# Close the file\n",
    "ad_pickle.close()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
